{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 09:29:05.533280: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-16 09:29:05.537103: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-16 09:29:05.588763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-16 09:29:06.443311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import UnidentifiedImageError\n",
    "from tensorflow.keras.preprocessing.image import (load_img, img_to_array, ImageDataGenerator)\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input, decode_predictions, VGG16)\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridsearch import gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parser():\n",
    "    \"\"\"\n",
    "    The user can specify whether to perform GridSearch, number of epochs, batch size, and data augmentation.\n",
    "    The function will then parse command-line arguments and make them lower case.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--GridSearch\",\n",
    "                        \"-gs\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform GridSearch (yes or no)\")\n",
    "    parser.add_argument(\"--BatchNorm\",\n",
    "                        \"-bn\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform batch normalization (yes or no)\")   \n",
    "    parser.add_argument(\"--DatAug\",\n",
    "                        \"-da\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform data augmentation (yes or no)\")                \n",
    "    args = parser.parse_args()\n",
    "    args.GridSearch = args.GridSearch.lower()\n",
    "    args.BatchNorm = args.BatchNorm.lower()\n",
    "    args.DatAug = args.DatAug.lower()\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def load_images(folder_path):\n",
    "    \"\"\"\n",
    "    Loads the data from the specified folder path, generates labels for each image, \n",
    "    and preprocesses them for model input.\n",
    "    The dataset contains certain files, i.e., Thumbs.db, could not be loaded and\n",
    "    returned the error 'UnidentifiedImageError'. These will simplt be ignored.\n",
    "    \"\"\"\n",
    "    list_of_images = [] \n",
    "    list_of_labels = []\n",
    "    \n",
    "    for subfolder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path  = os.path.join(folder_path, subfolder)\n",
    "        \n",
    "        for file in os.listdir(subfolder_path):\n",
    "            individual_filepath = os.path.join(subfolder_path, file)\n",
    "            \n",
    "            try:\n",
    "                image = load_img(individual_filepath, target_size = (224, 224))\n",
    "                image = img_to_array(image)\n",
    "                list_of_images.append(image)\n",
    "\n",
    "                label = subfolder_path.split(\"/\")[-1]\n",
    "                list_of_labels.append(label)\n",
    "\n",
    "            except (UnidentifiedImageError):\n",
    "                print(f\"Skipping {individual_filepath}\")\n",
    "        \n",
    "    array_of_images = np.array(list_of_images)\n",
    "    X = preprocess_input(array_of_images)\n",
    "    y = list_of_labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def data_split(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets by stratifing y.\n",
    "    Normalizes X to range between 0 and 1 and performs label binarization on y.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 123)\n",
    "    X_train = X_train.astype(\"float32\") / 255.\n",
    "    X_test = X_test.astype(\"float32\") / 255.\n",
    "    lb = LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.fit_transform(y_test) \n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def define_model(BatchNorm):\n",
    "    \"\"\"\n",
    "    Defines the model architecture. First, the VGG16 model is loaded from TensorFlow without the classification\n",
    "    layers and the convolutional layers are marked as not trainable to retain their pretrained weights.\n",
    "    Subsequently, a new fully connected layer with ReLU activation is added followed by an output layer with\n",
    "    softmax activation for multi-class classification.\n",
    "\n",
    "    batchNorm option --> user specifies whether the model should be defined with or without batch normalization\n",
    "\n",
    "    compile\n",
    "\n",
    "    \"\"\"\n",
    "    model = VGG16(include_top = False, pooling = 'avg', input_shape = (224, 224, 3))\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    if BatchNorm == \"no\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        class1 = Dense(128, activation = 'relu')(flat1)\n",
    "        output = Dense(10, activation = 'softmax')(class1)\n",
    "\n",
    "    if BatchNorm == \"yes\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        bn = BatchNormalization()(flat1)\n",
    "        class1 = Dense(128, activation='relu')(bn)\n",
    "        output = Dense(10, activation='softmax')(class1)\n",
    "    \n",
    "    model = Model(inputs = model.inputs, outputs = output)\n",
    "\n",
    "    # compile\n",
    "    #model.compile(loss = \"categorical_crossentropy\", metrics = [\"accuracy\"],\n",
    "    #            optimizer = optimizer, learning_rate = learning_rate,\n",
    "    #            epochs = epochs, batch_size = batch_size)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def sklearn_object(model):\n",
    "    \"\"\"\n",
    "    Convert the model from a KerasClassifier to an object, that can be used in a scikit-learn pipeline.\n",
    "    \"\"\"\n",
    "    model = KerasClassifier(model = model, verbose = 0)\n",
    "    return model\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    Creates an image data generator with data augmentation settings as horizontal flipping and rotation.\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(horizontal_flip = True, \n",
    "                                rotation_range = 90,\n",
    "                                validation_split = 0.1)\n",
    "    return datagen\n",
    "\n",
    "\n",
    "def compile_fit_model(model, X_train, y_train, model_config_default, DatAug):\n",
    "    \"\"\"\n",
    "    Compiles the model with the specified optimizer.\n",
    "\n",
    "    The user specifies whether to implement data augmentation...\n",
    "    Fits the compiled model to the training data with or without data augmentation and returns the training history.\n",
    "    \"\"\"\n",
    "    model.compile(loss = \"categorical_crossentropy\", metrics = [\"accuracy\"],\n",
    "                optimizer = optimizer, learning_rate = learning_rate,\n",
    "                epochs = epochs, batch_size = batch_size)\n",
    "    model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'].\n",
    "                optimizer = model_config_default.optimizer, learning_rate = model_config_default.learning_rate,\n",
    "                epochs = model_config_default.epochs, batch_size = model_config_default.batch_size)\n",
    "\n",
    "    if DatAug == \"no\":\n",
    "        H = model.fit(X_train, y_train, \n",
    "                    validation_split = 0.1,\n",
    "                    model_config_default,\n",
    "                    batch_size = model_config_default.batch_size, # 32\n",
    "                    epochs = model_config_default.epochs, #10\n",
    "                    verbose = 1)\n",
    "    elif DatAug == \"yes\":\n",
    "        DatAug = data_generator()\n",
    "        datagen.fit(X_train)\n",
    "        H = model.fit(datagen.flow(X_train, y_train, batch_size = BatchSize),\n",
    "                                    validation_data = datagen.flow(X_train, y_train, \n",
    "                                                                    batch_size = BatchSize,\n",
    "                                                                    subset = \"validation\"),\n",
    "                                                                    epochs = epochs) \n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(H, epochs, outpath):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and accuracy curves and saves the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"loss\"], label = \"train_loss\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label = \"val_loss\", linestyle = \":\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label = \"train_acc\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label = \"val_acc\", linestyle = \":\")\n",
    "    plt.title(\"Accuracy curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(outpath)\n",
    "\n",
    "\n",
    "def evaluate(X_test, y_test, model, H, BatchSize, epochs):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test data, generates classification reports, and saves the results.\n",
    "    \"\"\"\n",
    "    label_names = [\"ADVE\", \"Email\", \"Form\", \"Letter\", \"Memo\", \"News\", \"Note\", \"Report\", \"Resume\", \"Scientific\"]\n",
    "\n",
    "    predictions = model.predict(X_test, batch_size = BatchSize)\n",
    "\n",
    "    classifier_metrics = classification_report(y_test.argmax(axis = 1),\n",
    "                                               predictions.argmax(axis = 1),\n",
    "                                               target_names = label_names)\n",
    "\n",
    "    filepath_metrics = open('out/VGG16_metrics_BatchNorm.txt', 'w')\n",
    "    filepath_metrics.write(classifier_metrics)\n",
    "    filepath_metrics.close()\n",
    "\n",
    "    plot_history(H, epochs, \"out/VGG16_losscurve_BatchNorm.png\")\n",
    "\n",
    "    return print(\"Results have been saved to the out folder\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig_default:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.learning_rate = self.learning_rate_schedule()\n",
    "        self.epochs = 10\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def learning_rate_schedule(self):\n",
    "        return tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate = 0.001,\n",
    "            decay_steps = 10000,\n",
    "            decay_rate = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define model\n",
    "# - gridsearch on defined model? then it can be compiled with the best hyperparameters\n",
    "# compile model\n",
    "# - gridsearch on compiled model?\n",
    "# fit model\n",
    "\n",
    "# classes\n",
    "\n",
    "# add compile to define, ligesom week 12\n",
    "\n",
    "def main():\n",
    "    \n",
    "    args = parser()\n",
    "\n",
    "    folder_path = os.path.join(\"../../../../cds-vis-data/Tobacco3482\") # (\"in/Tobacco3482\")\n",
    "\n",
    "    X, y = load_images(folder_path)\n",
    "    X_train, X_test, y_train, y_test = data_split(X, y)\n",
    "    \n",
    "    # define model - BatchNorm yes/no\n",
    "    define_model(args.BatchNorm)\n",
    "\n",
    "    # grid search\n",
    "    if args.GridSearch == 'yes':\n",
    "        model = sklearn_object(model)\n",
    "        model = gridsearch(model, X_train, y_train)\n",
    "    else:\n",
    "        # load the default model\n",
    "        model = ModelConfig_default() # default (adam, default lr 0.001, 10 epochs, batch 32) \n",
    "\n",
    "    # compile with GridSearch parameters or default + fit model\n",
    "    H = compile_fit_model(model, X_train, y_train, model_config_default, args.DatAug)\n",
    "    # evaluate\n",
    "    evaluate(X_test, y_test, model, H)\n",
    "\n",
    "    # plotter \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(\"../../../../../cds-vis-data/Tobacco3482\") # (\"in/Tobacco3482\")\n",
    "\n",
    "X, y = load_images(folder_path)\n",
    "X_train, X_test, y_train, y_test = data_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functional name=functional_3, built=True>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "define_model('yes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
