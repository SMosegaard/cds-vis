{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from PIL import UnidentifiedImageError\n",
    "from tensorflow.keras.preprocessing.image import (load_img, img_to_array, ImageDataGenerator)\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input, decode_predictions, VGG16)\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridsearch import gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parser():\n",
    "    \"\"\"\n",
    "    The user can specify whether to perform GridSearch, number of epochs, batch size, and data augmentation.\n",
    "    The function will then parse command-line arguments and make them lower case.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--GridSearch\",\n",
    "                        \"-gs\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform GridSearch (yes or no)\")\n",
    "    parser.add_argument(\"--BatchNorm\",\n",
    "                        \"-bn\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform batch normalization (yes or no)\")   \n",
    "    parser.add_argument(\"--DatAug\",\n",
    "                        \"-da\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform data augmentation (yes or no)\")                \n",
    "    args = parser.parse_args()\n",
    "    args.GridSearch = args.GridSearch.lower()\n",
    "    args.BatchNorm = args.BatchNorm.lower()\n",
    "    args.DatAug = args.DatAug.lower()\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def load_images(folder_path):\n",
    "    \"\"\"\n",
    "    Loads the data from the specified folder path, generates labels for each image, \n",
    "    and preprocesses them for model input.\n",
    "    Certain images could not be loaded and returned the error 'UnidentifiedImageError'. These will simplt be ignored.\n",
    "    \"\"\"\n",
    "    list_of_images = [] \n",
    "    list_of_labels = []\n",
    "    \n",
    "    for subfolder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path  = os.path.join(folder_path, subfolder)\n",
    "        \n",
    "        for file in os.listdir(subfolder_path):\n",
    "            individual_filepath = os.path.join(subfolder_path, file)\n",
    "            \n",
    "            try:\n",
    "                image = load_img(individual_filepath, target_size = (224, 224))\n",
    "                image = img_to_array(image)\n",
    "                list_of_images.append(image)\n",
    "\n",
    "                label = subfolder_path.split(\"/\")[-1]\n",
    "                list_of_labels.append(label)\n",
    "\n",
    "            except (UnidentifiedImageError):\n",
    "                print(f\"Skipping {individual_filepath}\")\n",
    "        \n",
    "    array_of_images = np.array(list_of_images)\n",
    "    X = preprocess_input(array_of_images)\n",
    "    y = list_of_labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def data_split(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets by stratifing y.\n",
    "    Normalizes X and performs label binarization on y.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 123)\n",
    "    X_train = X_train.astype(\"float32\") / 255.\n",
    "    X_test = X_test.astype(\"float32\") / 255.\n",
    "    lb = LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.fit_transform(y_test) \n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def define_model(BatchNorm):\n",
    "    \"\"\"\n",
    "    Defines the model architecture. First, the VGG16 model is loaded without the classification layers and \n",
    "    the convolutional layers are marked as not trainable to retain their pretrained weights.\n",
    "    Subsequently, a new fully connected layer with ReLU activation is added followed by an output layer with\n",
    "    softmax activation for multi-class classification.\n",
    "    \"\"\"\n",
    "    model = VGG16(include_top = False, pooling = 'avg', input_shape = (224, 224, 3))\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    if BatchNorm == \"no\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        class1 = Dense(128, activation = 'relu')(flat1)\n",
    "        output = Dense(10, activation = 'softmax')(class1)\n",
    "\n",
    "    if BatchNorm == \"yes\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        bn = BatchNormalization()(flat1)\n",
    "        class1 = Dense(128, activation='relu')(bn)\n",
    "        output = Dense(10, activation='softmax')(class1)\n",
    "    \n",
    "    model = Model(inputs = model.inputs, outputs = output)\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss = \"categorical_crossentropy\", metrics = [\"accuracy\"],\n",
    "                optimizer = optimizer, learning_rate = learning_rate,\n",
    "                epochs = epochs, batch_size = batch_size)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def sklearn_object(model):\n",
    "    \"\"\"\n",
    "    Convert the model from a KerasClassifier to an object, that can be used in a scikit-learn pipeline.\n",
    "    \"\"\"\n",
    "    model = KerasClassifier(model = model, verbose = 0)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    \"\"\"\n",
    "    Compiles the model with the specified optimizer.\n",
    "    \"\"\"\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.001,\n",
    "                                                                 decay_steps = 10000,\n",
    "                                                                 decay_rate = 0.9)\n",
    "    \n",
    "    adam = Adam(learning_rate = lr_schedule)\n",
    "\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    Creates an image data generator with data augmentation settings as horizontal flipping and rotation.\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(horizontal_flip = True, \n",
    "                                rotation_range = 90,\n",
    "                                validation_split = 0.1)\n",
    "    return datagen\n",
    "\n",
    "\n",
    "def fit_model_DatAug(model, datagen, X_train, y_train, BatchSize, epochs):\n",
    "    \"\"\"\n",
    "    Fits the compiled model to the training data using data augmentation and returns the training history.\n",
    "    \"\"\"\n",
    "    datagen.fit(X_train)\n",
    "    H = model.fit(datagen.flow(X_train, y_train, batch_size = BatchSize),\n",
    "                               validation_data = datagen.flow(X_train, y_train, \n",
    "                                                              batch_size = BatchSize,\n",
    "                                                              subset = \"validation\"),\n",
    "                                                              epochs = epochs) \n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(model, X_train, y_train, BatchSize, epochs):\n",
    "    \"\"\"\n",
    "    Fits the compiled model to the training data and returns the training history.\n",
    "    \"\"\"\n",
    "    H = model.fit(X_train, y_train, \n",
    "                  validation_split = 0.1,\n",
    "                  batch_size = BatchSize,\n",
    "                  epochs = epochs,\n",
    "                  verbose = 1)\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(H, epochs, outpath):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and accuracy curves and saves the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"loss\"], label = \"train_loss\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label = \"val_loss\", linestyle = \":\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label = \"train_acc\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label = \"val_acc\", linestyle = \":\")\n",
    "    plt.title(\"Accuracy curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(outpath)\n",
    "\n",
    "\n",
    "def evaluate(X_test, y_test, model, H, BatchSize, epochs):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test data, generates classification reports, and saves the results.\n",
    "    \"\"\"\n",
    "    label_names = [\"ADVE\", \"Email\", \"Form\", \"Letter\", \"Memo\", \"News\", \"Note\", \"Report\", \"Resume\", \"Scientific\"]\n",
    "\n",
    "    predictions = model.predict(X_test, batch_size = BatchSize)\n",
    "\n",
    "    classifier_metrics = classification_report(y_test.argmax(axis = 1),\n",
    "                                               predictions.argmax(axis = 1),\n",
    "                                               target_names = label_names)\n",
    "\n",
    "    filepath_metrics = open('out/VGG16_metrics_BatchNorm.txt', 'w')\n",
    "    filepath_metrics.write(classifier_metrics)\n",
    "    filepath_metrics.close()\n",
    "\n",
    "    plot_history(H, epochs, \"out/VGG16_losscurve_BatchNorm.png\")\n",
    "\n",
    "    return print(\"Results have been saved to the out folder\")\n",
    "\n",
    "\n",
    "# define model\n",
    "# - gridsearch on defined model? then it can be compiled with the best hyperparameters\n",
    "# compile model\n",
    "# - gridsearch on compiled model?\n",
    "# fit model\n",
    "\n",
    "# classes\n",
    "\n",
    "# add compile to define, ligesom week 12\n",
    "\n",
    "def main():\n",
    "    \n",
    "    args = parser()\n",
    "\n",
    "    folder_path = os.path.join(\"../../../../cds-vis-data/Tobacco3482\") # (\"in/Tobacco3482\")\n",
    "\n",
    "    X, y = load_images(folder_path)\n",
    "    X_train, X_test, y_train, y_test = data_split(X, y)\n",
    "    \n",
    "    # define model\n",
    "    define_model(args.BatchNorm)\n",
    "\n",
    "    # grid search\n",
    "    if args.GridSearch == 'yes':\n",
    "        model = sklearn_object(model)\n",
    "        model = gridsearch(model, X_train, y_train)\n",
    "    else:\n",
    "        continue # default (adam, default lr 0.001, 10 epochs, batch 32) \n",
    "\n",
    "    # fit model\n",
    "    if args.DatAug == 'yes':\n",
    "        datagen = data_generator() \n",
    "        H = fit_model_DatAug(model, datagen, X_train, y_train)\n",
    "    else:\n",
    "        H = fit_model(model, X_train, y_train)\n",
    "\n",
    "    evaluate(X_test, y_test, model, H)\n",
    "\n",
    "    # plotter \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
