{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:33:15.178094: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-16 13:33:17.986159: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-16 13:33:27.297943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-16 13:34:20.190393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m UnidentifiedImageError\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m (load_img, img_to_array, ImageDataGenerator)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/__init__.py:51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m autograph\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[1;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/_api/v2/compat/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m v1\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m v2\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_compatibility_horizon \u001b[39m# line: 125\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/__init__.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m autograph\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m bitwise\n\u001b[0;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m v1\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m v2\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_compatibility_horizon \u001b[39m# line: 125\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m debugging\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import UnidentifiedImageError\n",
    "from tensorflow.keras.preprocessing.image import (load_img, img_to_array, ImageDataGenerator)\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input, decode_predictions, VGG16)\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridsearch import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parser():\n",
    "    \"\"\"\n",
    "    The user can specify whether to perform GridSearch, implement batch normalization and/or data augmentation,\n",
    "    .\n",
    "    The function will then parse command-line arguments and make them lower case.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--GridSearch\",\n",
    "                        \"-gs\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform GridSearch (yes or no)\")\n",
    "    parser.add_argument(\"--BatchNorm\",\n",
    "                        \"-bn\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform batch normalization (yes or no)\")   \n",
    "    parser.add_argument(\"--DatAug\",\n",
    "                        \"-da\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform data augmentation (yes or no)\")    \n",
    "    parser.add_argument(\"--optimizer\",\n",
    "                        \"-o\",\n",
    "                        required = True,\n",
    "                        choices = [\"adam\", \"sgd\"],\n",
    "                        help = \"Choose optimizer\")              \n",
    "    args = parser.parse_args()\n",
    "    args.GridSearch = args.GridSearch.lower()\n",
    "    args.BatchNorm = args.BatchNorm.lower()\n",
    "    args.DatAug = args.DatAug.lower()\n",
    "    args.optimizer = args.optimizer.lower()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_images(folder_path):\n",
    "    \"\"\"\n",
    "    Loads the data from the specified folder path, generates labels for each image, \n",
    "    and preprocesses them for model input.\n",
    "    The dataset contains certain files, i.e., Thumbs.db, could not be loaded and\n",
    "    returned the error 'UnidentifiedImageError'. These will simplt be ignored.\n",
    "    \"\"\"\n",
    "    list_of_images = [] \n",
    "    list_of_labels = []\n",
    "    \n",
    "    for subfolder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path  = os.path.join(folder_path, subfolder)\n",
    "        \n",
    "        for file in os.listdir(subfolder_path):\n",
    "            individual_filepath = os.path.join(subfolder_path, file)\n",
    "            \n",
    "            try:\n",
    "                image = load_img(individual_filepath, target_size = (224, 224))\n",
    "                image = img_to_array(image)\n",
    "                list_of_images.append(image)\n",
    "\n",
    "                label = subfolder_path.split(\"/\")[-1]\n",
    "                list_of_labels.append(label)\n",
    "\n",
    "            except (UnidentifiedImageError):\n",
    "                print(f\"Skipping {individual_filepath}\")\n",
    "        \n",
    "    array_of_images = np.array(list_of_images)\n",
    "    X = preprocess_input(array_of_images)\n",
    "    y = list_of_labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def data_split(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets by stratifing y.\n",
    "    Normalizes X to range between 0 and 1 and performs label binarization on y.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 123)\n",
    "    X_train = X_train.astype(\"float32\") / 255.\n",
    "    X_test = X_test.astype(\"float32\") / 255.\n",
    "    lb = LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.fit_transform(y_test) \n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def define_model(BatchNorm, Optimizer):\n",
    "    \"\"\"\n",
    "    Defines the model architecture. First, the VGG16 model is loaded from TensorFlow without the classification\n",
    "    layers and the convolutional layers are marked as not trainable to retain their pretrained weights.\n",
    "    Subsequently, a new fully connected layer with ReLU activation is added followed by an output layer with\n",
    "    softmax activation for multi-class classification.\n",
    "\n",
    "    batchNorm option --> user specifies whether the model should be defined with or without batch normalization\n",
    "\n",
    "    Compiles the model with the specified optimizer ... learning rate \n",
    "\n",
    "    \"\"\"\n",
    "    model = VGG16(include_top = False, pooling = 'avg', input_shape = (224, 224, 3))\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    if BatchNorm == \"no\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        class1 = Dense(128, activation = 'relu')(flat1)\n",
    "        output = Dense(10, activation = 'softmax')(class1)\n",
    "\n",
    "    elif BatchNorm == \"yes\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        bn = BatchNormalization()(flat1)\n",
    "        class1 = Dense(128, activation='relu')(bn)\n",
    "        output = Dense(10, activation='softmax')(class1)\n",
    "    \n",
    "    model = Model(inputs = model.inputs, outputs = output)\n",
    "\n",
    "    # compile\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.01, \n",
    "                                                                decay_steps = 10000, decay_rate = 0.9)\n",
    "    if Optimizer == \"adam\":\n",
    "        optimizer = Adam(learning_rate = lr_schedule)\n",
    "    if Optimizer == \"sgd\":\n",
    "        optimizer = SGD(learning_rate = lr_schedule)\n",
    "    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def sklearn_object(model):\n",
    "    \"\"\"\n",
    "    Convert the model from a KerasClassifier to an object, that can be used in a scikit-learn pipeline.\n",
    "    \"\"\"\n",
    "    model = KerasClassifier(model = model, verbose = 0)\n",
    "    return model\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    Creates an image data generator with data augmentation settings as horizontal flipping and rotation.\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(horizontal_flip = True, \n",
    "                                rotation_range = 90,\n",
    "                                validation_split = 0.1)\n",
    "    return datagen\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(model, X_train, y_train, DatAug):\n",
    "    \"\"\"\n",
    "    The user specifies whether to implement data augmentation...\n",
    "    Fits the compiled model to the training data with or without data augmentation and returns the training history.\n",
    "\n",
    "    early stopping implemented to minimise loss and avoid overfitting. Monitors val_loss\n",
    "    After 3 epochs with no improvement, the training will be stopped\n",
    "    \"\"\"\n",
    "\n",
    "    callback = [keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 3)]\n",
    "\n",
    "    if DatAug == \"no\":\n",
    "        H = model.fit(X_train, y_train, \n",
    "                    validation_split = 0.1,\n",
    "                    batch_size = 30,\n",
    "                    epochs = epochs,\n",
    "                    callbacks = callbacks)\n",
    "    \n",
    "    elif DatAug == \"yes\":\n",
    "        DatAug = data_generator()\n",
    "        datagen.fit(X_train)\n",
    "        H = model.fit(datagen.flow(X_train, y_train, batch_size = BatchSize),\n",
    "                                    validation_data = datagen.flow(X_train, y_train, \n",
    "                                                                    batch_size = BatchSize,\n",
    "                                                                    subset = \"validation\"),\n",
    "                                                                    epochs = epochs,\n",
    "                                                                    callbacks = callbacks) \n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "def plot_history(H, epochs, outpath):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and accuracy curves and saves the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"loss\"], label = \"train_loss\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label = \"val_loss\", linestyle = \":\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label = \"train_acc\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label = \"val_acc\", linestyle = \":\")\n",
    "    plt.title(\"Accuracy curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(outpath)\n",
    "\n",
    "\n",
    "def evaluate(X_test, y_test, model, H, BatchSize, epochs):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test data, generates classification reports, and saves the results.\n",
    "    \"\"\"\n",
    "    label_names = [\"ADVE\", \"Email\", \"Form\", \"Letter\", \"Memo\", \"News\", \"Note\", \"Report\", \"Resume\", \"Scientific\"]\n",
    "\n",
    "    predictions = model.predict(X_test, batch_size = BatchSize)\n",
    "\n",
    "    classifier_metrics = classification_report(y_test.argmax(axis = 1),\n",
    "                                               predictions.argmax(axis = 1),\n",
    "                                               target_names = label_names)\n",
    "\n",
    "    filepath_metrics = open('out/VGG16_metrics_BatchNorm.txt', 'w')\n",
    "    filepath_metrics.write(classifier_metrics)\n",
    "    filepath_metrics.close()\n",
    "\n",
    "    plot_history(H, epochs, \"out/VGG16_losscurve_BatchNorm.png\")\n",
    "\n",
    "    return print(\"Results have been saved to the out folder\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    args = parser()\n",
    "\n",
    "    folder_path = os.path.join(\"../../../../cds-vis-data/Tobacco3482\") # (\"in/Tobacco3482\")\n",
    "\n",
    "    X, y = load_images(folder_path)\n",
    "    X_train, X_test, y_train, y_test = data_split(X, y)\n",
    "    \n",
    "    # define model - BatchNorm yes/no\n",
    "    model = define_model(args.BatchNorm, args.Optimizer)\n",
    "\n",
    "    # grid search\n",
    "    if args.GridSearch == 'yes':\n",
    "        model = sklearn_object(model)\n",
    "        model = gridsearch(model, X_train, y_train)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # fit model\n",
    "    H = fit_model(model, X_train, y_train, args.DatAug)\n",
    "    \n",
    "    # evaluate\n",
    "    evaluate(X_test, y_test, model, H)\n",
    "\n",
    "    # plotter \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ../../../../../cds-vis-data/Tobacco3482/ADVE/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Email/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Form/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Letter/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Memo/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/News/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Note/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Report/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Resume/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Scientific/Thumbs.db\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(\"../../../../../cds-vis-data/Tobacco3482\") # (\"in/Tobacco3482\")\n",
    "\n",
    "X, y = load_images(folder_path)\n",
    "X_train, X_test, y_train, y_test = data_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(\"yes\", \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functional name=functional_1, built=True>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train, DatAug, batch_size = None, epochs = None):\n",
    "\n",
    "    if batch_size is None:\n",
    "        batch_size = 32\n",
    "    if epochs is None:\n",
    "        epochs = 10\n",
    "\n",
    "    if DatAug == \"no\":\n",
    "\n",
    "        H = model.fit(X_train, y_train,\n",
    "                      validation_split = 0.1,\n",
    "                      batch_size = batch_size,\n",
    "                      epochs = epochs,\n",
    "                      verbose = 1)\n",
    "\n",
    "    elif DatAug == \"yes\":\n",
    "\n",
    "        datagen = data_generator()\n",
    "        datagen.fit(X_train)\n",
    "        H = model.fit(datagen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                      validation_data = datagen.flow(X_train, y_train,\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  subset = \"validation\"),\n",
    "                                                  epochs = epochs,\n",
    "                                                  verbose = 1)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 3s/step - accuracy: 0.7730 - loss: 0.6499 - val_accuracy: 0.6703 - val_loss: 1.0609\n",
      "Epoch 2/2\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 3s/step - accuracy: 0.7949 - loss: 0.6002 - val_accuracy: 0.7240 - val_loss: 1.0664\n"
     ]
    }
   ],
   "source": [
    "H = fit_model(model, X_train, y_train, \"no\", batch_size = None, epochs = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = x_train[np.random.choice(x_train.shape[0], 10, replace=False)]\n",
    "explainer = shap.DeepExplainer(model, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n",
      "2024-05-16 13:31:39.303787: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 35773480960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "e = shap.DeepExplainer(model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "<class 'keras.src.callbacks.history.History'> is not currently a supported model type!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mGradientExplainer(H, X_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:83\u001b[0m, in \u001b[0;36mGradientExplainer.__init__\u001b[0;34m(self, model, data, session, batch_size, local_smoothing)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m framework \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer \u001b[39m=\u001b[39m _TFGradient(model, data, session, batch_size, local_smoothing)\n\u001b[1;32m     84\u001b[0m \u001b[39melif\u001b[39;00m framework \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer \u001b[39m=\u001b[39m _PyTorchGradient(model, data, batch_size, local_smoothing)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:181\u001b[0m, in \u001b[0;36m_TFGradient.__init__\u001b[0;34m(self, model, data, session, batch_size, local_smoothing)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39m# determine the model inputs and outputs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs \u001b[39m=\u001b[39m _get_model_inputs(model)\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m _get_model_output(model)\n\u001b[1;32m    183\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output, \u001b[39mlist\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mThe model output to be explained must be a single tensor!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/tf_utils.py:73\u001b[0m, in \u001b[0;36m_get_model_inputs\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m model[\u001b[39m0\u001b[39m]\n\u001b[1;32m     72\u001b[0m emsg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m}\u001b[39;00m\u001b[39m is not currently a supported model type!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(emsg)\n",
      "\u001b[0;31mValueError\u001b[0m: <class 'keras.src.callbacks.history.History'> is not currently a supported model type!"
     ]
    }
   ],
   "source": [
    "explainer = shap.GradientExplainer(H, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/home/ucloud/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py\", line 231, in grad_graph  *\n        phase = tf.keras.backend.learning_phase()\n\n    AttributeError: module 'keras._tf_keras.keras.backend' has no attribute 'learning_phase'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(X_test[:\u001b[39m5\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:158\u001b[0m, in \u001b[0;36mGradientExplainer.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, nsamples\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, ranked_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_rank_order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, rseed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, return_variances\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    109\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the values for the model applied to X.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mshap_values(X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:322\u001b[0m, in \u001b[0;36m_TFGradient.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, nsamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[1;32m    321\u001b[0m     batch \u001b[39m=\u001b[39m [samples_input[a][b:\u001b[39mmin\u001b[39m(b\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,nsamples)] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))]\n\u001b[0;32m--> 322\u001b[0m     grads\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(find), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_inputs, batch))\n\u001b[1;32m    323\u001b[0m grad \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mconcatenate([g[a] \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads], \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))]\n\u001b[1;32m    325\u001b[0m \u001b[39m# assign the attributions to the right part of the output arrays\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:395\u001b[0m, in \u001b[0;36m_TFGradient.run\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    393\u001b[0m     v \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(X[i]\u001b[39m.\u001b[39mreshape(shape), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs[i]\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    394\u001b[0m     inputs\u001b[39m.\u001b[39mappend(v)\n\u001b[0;32m--> 395\u001b[0m \u001b[39mreturn\u001b[39;00m out(inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filevx22jhn_.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__grad_graph\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 12\u001b[0m phase \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49mlearning_phase, (), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mset_learning_phase, (\u001b[39m0\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape(watch_accessed_variables\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m tape:\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/home/ucloud/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py\", line 231, in grad_graph  *\n        phase = tf.keras.backend.learning_phase()\n\n    AttributeError: module 'keras._tf_keras.keras.backend' has no attribute 'learning_phase'\n"
     ]
    }
   ],
   "source": [
    "shap_values = explainer.shap_values(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/home/ucloud/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py\", line 231, in grad_graph  *\n        phase = tf.keras.backend.learning_phase()\n\n    AttributeError: module 'keras._tf_keras.keras.backend' has no attribute 'learning_phase'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mGradientExplainer(model, X_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(X_test[:\u001b[39m10\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(shap_values))\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5048532-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/combine.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m shap\u001b[39m.\u001b[39mimage_plot([shap_values[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)], X_test[:\u001b[39m10\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:158\u001b[0m, in \u001b[0;36mGradientExplainer.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, nsamples\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, ranked_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_rank_order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, rseed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, return_variances\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    109\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the values for the model applied to X.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mshap_values(X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:322\u001b[0m, in \u001b[0;36m_TFGradient.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, nsamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[1;32m    321\u001b[0m     batch \u001b[39m=\u001b[39m [samples_input[a][b:\u001b[39mmin\u001b[39m(b\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,nsamples)] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))]\n\u001b[0;32m--> 322\u001b[0m     grads\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(find), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_inputs, batch))\n\u001b[1;32m    323\u001b[0m grad \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mconcatenate([g[a] \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads], \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))]\n\u001b[1;32m    325\u001b[0m \u001b[39m# assign the attributions to the right part of the output arrays\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py:395\u001b[0m, in \u001b[0;36m_TFGradient.run\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    393\u001b[0m     v \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(X[i]\u001b[39m.\u001b[39mreshape(shape), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs[i]\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    394\u001b[0m     inputs\u001b[39m.\u001b[39mappend(v)\n\u001b[0;32m--> 395\u001b[0m \u001b[39mreturn\u001b[39;00m out(inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filevx22jhn_.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__grad_graph\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 12\u001b[0m phase \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49mlearning_phase, (), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mset_learning_phase, (\u001b[39m0\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape(watch_accessed_variables\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m tape:\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/home/ucloud/.local/lib/python3.10/site-packages/shap/explainers/_gradient.py\", line 231, in grad_graph  *\n        phase = tf.keras.backend.learning_phase()\n\n    AttributeError: module 'keras._tf_keras.keras.backend' has no attribute 'learning_phase'\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.GradientExplainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_test[:10])\n",
    "\n",
    "print(len(shap_values))\n",
    "shap.image_plot([shap_values[i] for i in range(10)], X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search(model, X_train, y_train):\n",
    "    \n",
    "    param_grid = {'epochs': [2],\n",
    "                'batch_size': [16]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, cv = 2, n_jobs = -1,\n",
    "                                scoring = 'accuracy', verbose = 1)\n",
    "\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Best Accuracy for {grid_result.best_score_} using the parameters {grid_result.best_params_}')\n",
    "\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(f' mean = {mean:.4}, std = {stdev:.4} using {param}')\n",
    "\n",
    "    best_estimator = grid_result.best_estimator_\n",
    "\n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig_default:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimizer = adam\n",
    "        self.learning_rate = self.learning_rate_schedule()\n",
    "        self.epochs = 10\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def learning_rate_schedule(self):\n",
    "        return tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate = 0.001,\n",
    "            decay_steps = 10000,\n",
    "            decay_rate = 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
