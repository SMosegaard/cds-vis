{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 11:22:28.852832: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 11:22:28.856621: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 11:22:28.908530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 11:22:29.817726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/Visual_analytics/cds-vis/assignments/assignment-3/src/test.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5049172-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5049172-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelBinarizer\n\u001b[0;32m---> <a href='vscode-notebook-cell://app-5049172-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5049172-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split, GridSearchCV\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5049172-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/src/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.metrics` module includes score functions, performance metrics\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mand pairwise metrics and distance computations.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m cluster\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_classification\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     accuracy_score,\n\u001b[1;32m     10\u001b[0m     balanced_accuracy_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     zero_one_loss,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_dist_metrics\u001b[39;00m \u001b[39mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/cluster/__init__.py:25\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bicluster\u001b[39;00m \u001b[39mimport\u001b[39;00m consensus_score\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_supervised\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     adjusted_mutual_info_score,\n\u001b[1;32m     11\u001b[0m     adjusted_rand_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     v_measure_score,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_unsupervised\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     calinski_harabasz_score,\n\u001b[1;32m     27\u001b[0m     davies_bouldin_score,\n\u001b[1;32m     28\u001b[0m     silhouette_samples,\n\u001b[1;32m     29\u001b[0m     silhouette_score,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39madjusted_mutual_info_score\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnormalized_mutual_info_score\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconsensus_score\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:22\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _safe_indexing, check_random_state, check_X_y\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     Interval,\n\u001b[1;32m     19\u001b[0m     StrOptions,\n\u001b[1;32m     20\u001b[0m     validate_params,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m _VALID_METRICS, pairwise_distances, pairwise_distances_chunked\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_number_of_labels\u001b[39m(n_labels, n_samples):\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m        Number of samples.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m _num_samples, check_non_negative\n\u001b[0;32m---> 43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_pairwise_distances_reduction\u001b[39;00m \u001b[39mimport\u001b[39;00m ArgKmin\n\u001b[1;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_pairwise_fast\u001b[39;00m \u001b[39mimport\u001b[39;00m _chi2_kernel_fast, _sparse_manhattan\n\u001b[1;32m     47\u001b[0m \u001b[39m# Utility Functions\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py:94\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Pairwise Distances Reductions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# =============================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m#    (see :class:`MiddleTermComputer{32,64}`).\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_dispatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     ArgKmin,\n\u001b[1;32m     96\u001b[0m     ArgKminClassMode,\n\u001b[1;32m     97\u001b[0m     BaseDistancesReductionDispatcher,\n\u001b[1;32m     98\u001b[0m     RadiusNeighbors,\n\u001b[1;32m     99\u001b[0m     RadiusNeighborsClassMode,\n\u001b[1;32m    100\u001b[0m     sqeuclidean_row_norms,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBaseDistancesReductionDispatcher\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mArgKmin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msqeuclidean_row_norms\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m ]\n\u001b[1;32m    112\u001b[0m \u001b[39m# ruff: noqa: E501\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:13\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dist_metrics\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     BOOL_METRICS,\n\u001b[1;32m     10\u001b[0m     METRIC_MAPPING64,\n\u001b[1;32m     11\u001b[0m     DistanceMetric,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_argkmin\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ArgKmin32,\n\u001b[1;32m     15\u001b[0m     ArgKmin64,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_argkmin_classmode\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     ArgKminClassMode32,\n\u001b[1;32m     19\u001b[0m     ArgKminClassMode64,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m _sqeuclidean_row_norms32, _sqeuclidean_row_norms64\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.metrics._pairwise_distances_reduction._argkmin\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_base.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.metrics._pairwise_distances_reduction._base\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import UnidentifiedImageError\n",
    "from tensorflow.keras.preprocessing.image import (load_img, img_to_array, ImageDataGenerator)\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input, decode_predictions, VGG16)\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parser():\n",
    "    \"\"\"\n",
    "    The user can specify which optimizer to use, whether to perform GridSearch to tune the hyperparameters, and\n",
    "    to implement batch normalization and/or data augmentation.\n",
    "    The function will then parse command-line arguments and make them lower case.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--optimizer\",\n",
    "                        \"-o\",\n",
    "                        required = True,\n",
    "                        choices = [\"adam\", \"sgd\"],\n",
    "                        help = \"Choose optimizer\") \n",
    "    parser.add_argument(\"--GridSearch\",\n",
    "                        \"-gs\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform GridSearch (yes or no)\")\n",
    "    parser.add_argument(\"--BatchNorm\",\n",
    "                        \"-bn\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform batch normalization (yes or no)\")   \n",
    "    parser.add_argument(\"--DatAug\",\n",
    "                        \"-da\",\n",
    "                        required = True,\n",
    "                        choices = [\"yes\", \"no\"],\n",
    "                        help = \"Perform data augmentation (yes or no)\")                 \n",
    "    args = parser.parse_args()\n",
    "    args.optimizer = args.optimizer.lower()\n",
    "    args.GridSearch = args.GridSearch.lower()\n",
    "    args.BatchNorm = args.BatchNorm.lower()\n",
    "    args.DatAug = args.DatAug.lower()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_images(folder_path):\n",
    "    \"\"\"\n",
    "    Loads the data from the specified folder path, generates labels for each image, \n",
    "    and preprocesses them for model input.\n",
    "    The dataset contains certain files, i.e., Thumbs.db, could not be loaded and\n",
    "    returned the error 'UnidentifiedImageError'. These will simplt be ignored.\n",
    "    \"\"\"\n",
    "    list_of_images = [] \n",
    "    list_of_labels = []\n",
    "    \n",
    "    for subfolder in sorted(os.listdir(folder_path)):\n",
    "        subfolder_path  = os.path.join(folder_path, subfolder)\n",
    "        \n",
    "        for file in os.listdir(subfolder_path):\n",
    "            individual_filepath = os.path.join(subfolder_path, file)\n",
    "            \n",
    "            try:\n",
    "                image = load_img(individual_filepath, target_size = (224, 224))\n",
    "                image = img_to_array(image)\n",
    "                list_of_images.append(image)\n",
    "\n",
    "                label = subfolder_path.split(\"/\")[-1]\n",
    "                list_of_labels.append(label)\n",
    "\n",
    "            except (UnidentifiedImageError):\n",
    "                print(f\"Skipping {individual_filepath}\")\n",
    "        \n",
    "    array_of_images = np.array(list_of_images)\n",
    "    X = preprocess_input(array_of_images)\n",
    "    y = list_of_labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def data_split(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets (80:20 split) by stratifing y. Normalizes X\n",
    "    by simply dividing by the maximum possible value and performs label binarization on y.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 123)\n",
    "    X_train = X_train.astype(\"float32\") / 255.\n",
    "    X_test = X_test.astype(\"float32\") / 255.\n",
    "    lb = LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.fit_transform(y_test) \n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def define_model(BatchNorm):\n",
    "    \"\"\"\n",
    "    Defines the model architecture. First, the VGG16 model is loaded from TensorFlow without the classification\n",
    "    layers. The convolutional layers are marked as not trainable to retain their pretrained weights. The\n",
    "    user specifies whether the model should be defined with or without batch normalization. Subsequently, a\n",
    "    new fully connected layer with ReLU activation is added followed by an output layer with softmax\n",
    "    activation for multi-class classification.\n",
    "    \"\"\"\n",
    "    model = VGG16(include_top = False, pooling = 'avg', input_shape = (224, 224, 3))\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    if BatchNorm == \"no\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        class1 = Dense(128, activation = 'relu')(flat1)\n",
    "        output = Dense(10, activation = 'softmax')(class1)\n",
    "\n",
    "    elif BatchNorm == \"yes\":\n",
    "        flat1 = Flatten()(model.layers[-1].output)\n",
    "        bn = BatchNormalization()(flat1)\n",
    "        class1 = Dense(128, activation='relu')(bn)\n",
    "        output = Dense(10, activation='softmax')(class1)\n",
    "    \n",
    "    model = Model(inputs = model.inputs, outputs = output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sklearn_object(model):\n",
    "    \"\"\"\n",
    "    Convert the model from a KerasClassifier to an object, that can be used in a scikit-learn pipeline.\n",
    "    \"\"\"\n",
    "    #return KerasClassifier(model = model, verbose = 0)\n",
    "    return KerasClassifier(build_fn = model, verbose = 0)\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    The function creates an image data generator with ImageDataGenerator from TensorFlow.\n",
    "    The implemented data augmentation involves settings as horizontal flipping and rotation.\n",
    "    A validation split is set to 10%.\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(horizontal_flip = True, \n",
    "                                rotation_range = 90,\n",
    "                                validation_split = 0.1)\n",
    "    return datagen\n",
    "\n",
    "\n",
    "def fit_model(model, X_train, y_train, DatAug, batchsize = 32, epochs = 10):\n",
    "#def fit_model(model, X_train, y_train, DatAug, batchsize = batchsize, epochs = epochs):\n",
    "    \"\"\"\n",
    "    The function fits the defined and compiled model to the training data.\n",
    "\n",
    "    If the parameters have not been gridsearch to obtain the best parameters, the model will use a batch\n",
    "    size of 32 and 10 epochs as default. \n",
    "\n",
    "    The user specifies whether to implement data augmentation...\n",
    "    Fits the compiled model to the training data with or without data augmentation and returns the training history.\n",
    "\n",
    "    early stopping implemented to minimise loss and avoid overfitting. Monitors val_loss\n",
    "    After 3 epochs with no improvement, the training will be stopped\n",
    "    \n",
    "    When the model is fitted the function returns the fitted model, H.\n",
    "    \"\"\"\n",
    "\n",
    "    if DatAug == \"no\":\n",
    "        H = model.fit(X_train, y_train, \n",
    "                    validation_split = 0.1,\n",
    "                    batch_size = batchsize,\n",
    "                    epochs = epochs,\n",
    "                    callbacks = EarlyStopping(monitor = 'val_loss', patience = 3, mode='min'))\n",
    "    \n",
    "    elif DatAug == \"yes\":\n",
    "        DatAug = data_generator()\n",
    "        datagen.fit(X_train)\n",
    "        H = model.fit(datagen.flow(X_train, y_train, batch_size = batchsize),\n",
    "                                    validation_data = datagen.flow(X_train, y_train, \n",
    "                                                                    batch_size = batchsize,\n",
    "                                                                    subset = \"validation\"),\n",
    "                                                                    epochs = epochs,\n",
    "                                                                    callbacks = EarlyStopping(monitor = 'val_loss', patience = 3, mode='min')) \n",
    "    return H, batchsize, epochs\n",
    "\n",
    "\n",
    "def plot_history(H, epochs, model_param, Optimizer, outpath):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and accuracy curves and saves the plot to a specified outpath.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize = (12,6))\n",
    "    plt.suptitle(f\"Training and validation curves with {model_param} paramters and {Optimizer} optimizer\", fontsize = 8)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"loss\"], label = \"training loss\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label = \"validation loss\", linestyle = \":\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label = \"training accuracy\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label = \"validation accuracy\", linestyle = \":\")\n",
    "    plt.title(\"Accuracy curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(outpath)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate(model, X_test, y_test, H, batchsize, epochs, BatchNorm, DatAug, Optimizer):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test data, generates classification reports, and saves the results.\n",
    "    \"\"\"\n",
    "    label_names = [\"ADVE\", \"Email\", \"Form\", \"Letter\", \"Memo\", \"News\", \"Note\", \"Report\", \"Resume\", \"Scientific\"]\n",
    "    predictions = model.predict(X_test, batch_size = batchsize)\n",
    "    #y_preds = model.predict(X_test)\n",
    "    classifier_metrics = classification_report(y_test.argmax(axis = 1),\n",
    "                                               predictions.argmax(axis = 1),\n",
    "                                               target_names = label_names)\n",
    "\n",
    "    if BatchNorm == \"yes\":\n",
    "        if DatAug == \"yes\": \n",
    "            model_param = \"BatchNorm_DatAug\"\n",
    "        elif DatAug == \"no\":\n",
    "            model_param = \"BatchNorm\"\n",
    "    elif BatchNorm == \"no\": \n",
    "        if DatAug == \"yes\":\n",
    "            model_param = \"DatAug\"\n",
    "        elif DatAug == \"no\": \n",
    "            model_param = \"baseline\"\n",
    "\n",
    "    filepath_metrics = open(f'out/{model_param}_metrics_{Optimizer}.txt', 'w')\n",
    "    filepath_metrics.write(classifier_metrics)\n",
    "    filepath_metrics.close()\n",
    "\n",
    "    plot_history(H, epochs, model_param, Optimizer, f\"out/{model_param}_losscurve_{Optimizer}.png\")\n",
    "\n",
    "    return print(\"Results have been saved to the out folder\")\n",
    "\n",
    "\n",
    "def compile_model(model, Optimizer):\n",
    "    \"\"\"\n",
    "    The function compiles the model with the specified optimizer and respective learning rate\n",
    "    \"\"\"\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.01, \n",
    "                                                                decay_steps = 10000, decay_rate = 0.9)\n",
    "    if Optimizer == \"adam\":\n",
    "        optimizer = Adam(learning_rate = lr_schedule)\n",
    "    if Optimizer == \"sgd\":\n",
    "        optimizer = SGD(learning_rate = lr_schedule)\n",
    "    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return model \n",
    "\n",
    "def grid_search(model, X_train, y_train): #Optimizer\n",
    "    \"\"\"\n",
    "    The function initially converts the model from a KerasClassifier to an object, that can be used in a\n",
    "    scikit-learn pipeline. Afterwards, it performs GridSearch to find the best hyperparameters for the model.\n",
    "    The best parameters will be returned.\n",
    "    \"\"\"\n",
    "    #lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.01, \n",
    "    #                                                            decay_steps = 10000, decay_rate = 0.9)\n",
    "    #if Optimizer == \"adam\":\n",
    "    #    optimizer = Adam(learning_rate = lr_schedule)\n",
    "    #if Optimizer == \"sgd\":\n",
    "    #    optimizer = SGD(learning_rate = lr_schedule)\n",
    "    #model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    model = KerasClassifier(model = model, verbose = 1)\n",
    "\n",
    "    param_grid = {'epochs': [2],'batch_size': [16, 32]}\n",
    "    #param_grid = {'epochs': [10, 15, 20],\n",
    "    #            'batch_size': [16, 32, 64]}\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, cv = 2, n_jobs = -1, scoring = 'accuracy', verbose = 1)\n",
    "\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Best Accuracy for {grid_result.best_score_} using the parameters {grid_result.best_params_}')\n",
    "\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(f' mean = {mean:.4}, std = {stdev:.4} using {param}')\n",
    "\n",
    "    best_estimator = grid_result.best_estimator_\n",
    "    batchsize, epochs = list(grid_result.best_params_.values())\n",
    "    return best_estimator, batchsize, epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    args = parser()\n",
    "\n",
    "    folder_path = os.path.join(\"../../../../cds-vis-data/Tobacco3482\") # (\"in/Tobacco3482\")\n",
    "\n",
    "    X, y = load_images(folder_path)\n",
    "    X_train, X_test, y_train, y_test = data_split(X, y)\n",
    "    \n",
    "    # define model - BatchNorm yes/no\n",
    "    model = define_model(args.BatchNorm)\n",
    "\n",
    "    # compile - choose optimizer\n",
    "    model = compile_model(model, args.Optimizer)\n",
    "\n",
    "    # grid search + fit\n",
    "    if args.GridSearch == 'yes':\n",
    "        model, batchsize, epochs = grid_search(model, X_train, y_train)\n",
    "        H = fit_model(model, X_train, y_train, args.DatAug, batchsize = batchsize, epochs = epochs)\n",
    "    else:\n",
    "        H, batchsize, epochs = fit_model(model, X_train, y_train, args.DatAug, batchsize = 32, epochs = 10)\n",
    "  \n",
    "    # evaluate\n",
    "    evaluate(model, X_test, y_test, H, batchsize, epochs, args.BatchNorm, args.DatAug, args.Optimizer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ../../../../../cds-vis-data/Tobacco3482/ADVE/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Email/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Form/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Letter/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Memo/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/News/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Note/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Report/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Resume/Thumbs.db\n",
      "Skipping ../../../../../cds-vis-data/Tobacco3482/Scientific/Thumbs.db\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(\"../../../../../cds-vis-data/Tobacco3482\") # (\"in/Tobacco3482\")\n",
    "\n",
    "X, y = load_images(folder_path)\n",
    "X_train, X_test, y_train, y_test = data_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model(model, \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 9 variables whereas the saved optimizer has 1 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 10:53:10.706779: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:10.710761: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:10.759554: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 10:53:10.993007: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:10.997214: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:11.050310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 10:53:11.194875: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:11.198675: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:11.248246: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 10:53:11.353934: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:11.357557: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-17 10:53:11.405651: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 10:53:11.695161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-17 10:53:11.996008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-17 10:53:12.170633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-17 10:53:12.330286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "Epoch 1/2\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 15s/step - accuracy: 0.2944 - loss: 2.0118\n",
      "Epoch 2/2\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 15s/step - accuracy: 0.3145 - loss: 1.9759\n",
      "Epoch 2/2\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m827s\u001b[0m 10s/step - accuracy: 0.3298 - loss: 1.9328\n",
      "Epoch 2/2\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m827s\u001b[0m 9s/step - accuracy: 0.3544 - loss: 1.9109\n",
      "Epoch 2/2\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 14s/step - accuracy: 0.5620 - loss: 1.3032\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 15s/step - accuracy: 0.5403 - loss: 1.3208\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m827s\u001b[0m 10s/step - accuracy: 0.5677 - loss: 1.2415\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m851s\u001b[0m 9s/step - accuracy: 0.5235 - loss: 1.3015\n",
      "\u001b[1m 9/88\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:36\u001b[0m 8s/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/88\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:37\u001b[0m 7s/step "
     ]
    }
   ],
   "source": [
    "model, batchsize, epochs = grid_search(model, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
