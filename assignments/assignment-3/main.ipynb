{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 11:26:31.160124: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-11 11:26:31.257618: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-11 11:26:31.958758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 11:26:42.573783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "# image processsing\n",
    "from tensorflow.keras.preprocessing.image import (load_img,\n",
    "                                                  img_to_array,\n",
    "                                                  ImageDataGenerator)\n",
    "\n",
    "# VGG16 model\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input,\n",
    "                                                 decode_predictions,\n",
    "                                                 VGG16)\n",
    "\n",
    "# layers\n",
    "from tensorflow.keras.layers import (Flatten, \n",
    "                                     Dense, \n",
    "                                     Dropout, \n",
    "                                     BatchNormalization)\n",
    "\n",
    "# generic model object\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# optimizers\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(H, epochs, outpath):\n",
    "    plt.figure(figsize = (12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"loss\"], label = \"train_loss\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_loss\"], label = \"val_loss\", linestyle = \":\")\n",
    "    plt.title(\"Loss curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"accuracy\"], label = \"train_acc\")\n",
    "    plt.plot(np.arange(0, epochs), H.history[\"val_accuracy\"], label = \"val_acc\", linestyle = \":\")\n",
    "    plt.title(\"Accuracy curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ../../../../cds-vis-data/Tobacco3482/ADVE/Thumbs.db\n",
      "Skipping ../../../../cds-vis-data/Tobacco3482/Email/Thumbs.db\n",
      "Skipping ../../../../cds-vis-data/Tobacco3482/Form/Thumbs.db\n",
      "Skipping ../../../../cds-vis-data/Tobacco3482/Letter/Thumbs.db\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/Visual_analytics/cds-vis/assignments/assignment-3/main.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5035154-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m individual_filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(subfolder_path, file)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5035154-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5035154-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m# Load image from file\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://app-5035154-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         image \u001b[39m=\u001b[39m load_img(individual_filepath, target_size \u001b[39m=\u001b[39;49m (\u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5035154-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m# Convert the image pixels to a numpy array\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5035154-0.cloud.sdu.dk/work/Visual_analytics/cds-vis/assignments/assignment-3/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         image \u001b[39m=\u001b[39m img_to_array(image)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/image_utils.py:252\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39melif\u001b[39;00m color_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m         img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcolor_mode must be \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgrayscale\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrgba\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:941\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    891\u001b[0m     mode: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     colors: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m,\n\u001b[1;32m    896\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image:\n\u001b[1;32m    897\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    943\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\n\u001b[1;32m    944\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    945\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(\"../../../../cds-vis-data/Tobacco3482\")\n",
    "\n",
    "# Initialize empty lists\n",
    "list_of_images = [] \n",
    "list_of_labels = []\n",
    "\n",
    "# Loop through subfolders\n",
    "for subfolder in sorted(os.listdir(folder_path)):\n",
    "\n",
    "        # Extract images from the individual folders\n",
    "        subfolder_path  = os.path.join(folder_path, subfolder)\n",
    "\n",
    "        for file in os.listdir(subfolder_path):\n",
    "                \n",
    "                individual_filepath = os.path.join(subfolder_path, file)\n",
    "\n",
    "                try:\n",
    "                        # Load image from file\n",
    "                        image = load_img(individual_filepath, target_size = (224, 224))\n",
    "                        # Convert the image pixels to a numpy array\n",
    "                        image = img_to_array(image)\n",
    "                        list_of_images.append(image)\n",
    "\n",
    "                        # Extract image name and append to list\n",
    "                        #image_name = individual_filepath.split(\"/\")[-1]\n",
    "                        #list_of_images.append(image_name)\n",
    "\n",
    "                        # Extract label and append to list\n",
    "                        label = subfolder_path.split(\"/\")[-1]\n",
    "                        list_of_labels.append(label)\n",
    "                        \n",
    "                        # Append the preprocessed image along with its label as a tuple\n",
    "                        #list_of_images.append((image, label))\n",
    "\n",
    "                except (UnidentifiedImageError):\n",
    "                        print(f\"Skipping {individual_filepath}\")\n",
    "\n",
    "array_of_images = np.array(list_of_images)\n",
    "X = preprocess_input(array_of_images)\n",
    "y = list_of_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_path):\n",
    "        \n",
    "        # Initialize empty lists\n",
    "        list_of_images = [] \n",
    "        list_of_labels = []\n",
    "\n",
    "        for subfolder in sorted(os.listdir(folder_path)):\n",
    "                \n",
    "                # Extract images from the individual folders\n",
    "                subfolder_path  = os.path.join(folder_path, subfolder)\n",
    "                \n",
    "                for file in os.listdir(subfolder_path):\n",
    "                        \n",
    "                        individual_filepath = os.path.join(subfolder_path, file)\n",
    "                        \n",
    "                        try:\n",
    "                                # Load image from file\n",
    "                                image = load_img(individual_filepath, target_size = (224, 224))\n",
    "\n",
    "                                # Convert the image pixels to a numpy array\n",
    "                                image = img_to_array(image)\n",
    "\n",
    "                                list_of_images.append(image)\n",
    "\n",
    "                                # Extract image name and append to list\n",
    "                                #image_name = individual_filepath.split(\"/\")[-1]\n",
    "                                #list_of_images.append(image_name)\n",
    "\n",
    "                                # Extract label and append to list\n",
    "                                label = subfolder_path.split(\"/\")[-1]\n",
    "                                list_of_labels.append(label)\n",
    "                \n",
    "                        except (UnidentifiedImageError):\n",
    "                                print(f\"Skipping {individual_filepath}\")\n",
    "        \n",
    "        array_of_images = np.array(list_of_images)\n",
    "        X = preprocess_input(array_of_images)\n",
    "        y = list_of_labels\n",
    "        \n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 123)\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255.\n",
    "X_test = X_test.astype(\"float32\") / 255.\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)   # shape (2785, 10)\n",
    "y_test = lb.fit_transform(y_test)     # shape (697, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X,y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 123)\n",
    "    X_train = X_train.astype(\"float32\") / 255.\n",
    "    X_test = X_test.astype(\"float32\") / 255.\n",
    "    lb = LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.fit_transform(y_test) \n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kører alle\n",
    "```\n",
    "model = VGG16()\n",
    "\n",
    "# skal vi alle gøre - load model without classifier layers\n",
    "model = VGG16(include_top = False,        # dont use the final layer of the classifier\n",
    "              pooling = 'avg',\n",
    "              input_shape = (32, 32, 3))  # (height, width, number of color-channels)\n",
    "\n",
    "# skal vi alle gøre - mark loaded layers as not trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "```\n",
    "\n",
    "1. Ida kører clean model:\n",
    "```\n",
    "flat1 = Flatten()(model.layers[-1].output)  # take final layer (-1) in the model and append to flat1\n",
    "class1 = Dense(128, activation = 'relu')(flat1)\n",
    "output = Dense(10, activation = 'softmax')(class1)\n",
    "```\n",
    "\n",
    "2. Laura kører BatchNorm\n",
    "```\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "bn = BatchNormalization()(flat1)\n",
    "class1 = Dense(128, \n",
    "               activation='relu')(bn)\n",
    "output = Dense(10, \n",
    "               activation='softmax')(class2)\n",
    "```\n",
    "\n",
    "3. Jeg Sofie kører data argumentation - nedenstående\n",
    "\n",
    "4. Alle kører både sgd og adam!\n",
    "```\n",
    "sgd = SGD(learning_rate=lr_schedule)\n",
    "adam = Adam(learning_rate=lr_schedule)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model without classifier layers\n",
    "model = VGG16(include_top = False, \n",
    "              pooling = 'avg',\n",
    "              input_shape = (224, 224, 3)) \n",
    "\n",
    "# mark loaded layers as not trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "bn = BatchNormalization()(flat1)\n",
    "class2 = Dense(128, \n",
    "               activation='relu')(bn)\n",
    "output = Dense(10, \n",
    "               activation='softmax')(class2)\n",
    "\n",
    "# define new model\n",
    "model = Model(inputs = model.inputs, \n",
    "              outputs = output)\n",
    "\n",
    "# compile\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = 0.01,\n",
    "    decay_steps = 10000,\n",
    "    decay_rate = 0.9)\n",
    "    \n",
    "sgd = SGD(learning_rate=lr_schedule)\n",
    "#adam = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer = sgd,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "# summarize\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data generator\n",
    "\n",
    "# Add noise to an image\n",
    "def add_noise(image):\n",
    "    noise = np.random.normal(0, scale = 25, size = image.shape)\n",
    "    noisy_image = np.clip(image + noise, 0, 255) #.astype(np.uint8)\n",
    "    return noisy_image\n",
    "\n",
    "# flip along x axis (mirror image)\n",
    "datagen = ImageDataGenerator(horizontal_flip = True, \n",
    "                             rotation_range = 90,\n",
    "                             #preprocessing_function = add_noise(),\n",
    "                             validation_split = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 4s/step - accuracy: 0.2932 - loss: 2.0524 - val_accuracy: 0.2050 - val_loss: 2.0504\n",
      "Epoch 2/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 4s/step - accuracy: 0.5137 - loss: 1.4668 - val_accuracy: 0.3777 - val_loss: 1.8069\n",
      "Epoch 3/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 4s/step - accuracy: 0.5719 - loss: 1.2989 - val_accuracy: 0.5576 - val_loss: 1.5543\n",
      "Epoch 4/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 4s/step - accuracy: 0.5991 - loss: 1.2330 - val_accuracy: 0.6151 - val_loss: 1.3556\n",
      "Epoch 5/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 4s/step - accuracy: 0.6025 - loss: 1.2229 - val_accuracy: 0.6583 - val_loss: 1.2152\n",
      "Epoch 6/10\n",
      "\u001b[1m10/88\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:23\u001b[0m 4s/step - accuracy: 0.5742 - loss: 1.2619"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "# fit the data generator to our images\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "H = model.fit(datagen.flow(X_train, y_train, batch_size = 32), # change\n",
    "                            validation_data = datagen.flow(X_train, y_train, \n",
    "                                                           batch_size = 32,  # change\n",
    "                                                           subset = \"validation\"),\n",
    "                                                           epochs = 10)   # training for less time\n",
    "\n",
    "# first test batch 128, epoch 10\n",
    "# 1 epoch 358s = ca 6 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "plot_history(H, 10, \"out/VGG16_losscurve_data_augmentation_sgd-new.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 11s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ADVE       0.00      0.00      0.00        46\n",
      "       Email       0.00      0.00      0.00       120\n",
      "        Form       0.00      0.00      0.00        86\n",
      "      Letter       0.00      0.00      0.00       114\n",
      "        Memo       0.18      1.00      0.30       124\n",
      "        News       0.00      0.00      0.00        38\n",
      "        Note       0.00      0.00      0.00        40\n",
      "      Report       0.00      0.00      0.00        53\n",
      "      Resume       0.00      0.00      0.00        24\n",
      "  Scientific       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.18       697\n",
      "   macro avg       0.02      0.10      0.03       697\n",
      "weighted avg       0.03      0.18      0.05       697\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "label_names = [\"ADVE\", \"Email\", \"Form\", \"Letter\", \"Memo\", \"News\", \"Note\", \"Report\", \"Resume\", \"Scientific\"]\n",
    "\n",
    "predictions = model.predict(X_test, batch_size = 32)\n",
    "classifier_metrics = classification_report(y_test.argmax(axis = 1),\n",
    "                            predictions.argmax(axis = 1),\n",
    "                            target_names = label_names)\n",
    "\n",
    "\n",
    "print(classifier_metrics)\n",
    "\n",
    "filepath_metrics = open('out/VGG16_metrics_data_augmentation_sgd-new.txt', 'w')\n",
    "filepath_metrics.write(classifier_metrics)\n",
    "filepath_metrics.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(X_test, y_test, model, H, optimizer):\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    VGG16_metrics = classification_report(y_test.argmax(axis = 1),\n",
    "                                            predictions.argmax(axis = 1),\n",
    "                                            target_names = label_names)\n",
    "    filepath_metrics = open(f'out/VGG16_metrics{optimizer}.txt', 'w')\n",
    "    filepath_metrics.write(VGG16_metrics)\n",
    "    filepath_metrics.close()\n",
    "\n",
    "    plot_history(H, 5, f\"../out/VGG16_losscurve{optimizer}.png\")\n",
    "\n",
    "    return print(\"Results have been saved to the out folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_report = \"out/LR_classification_report.txt\"\n",
    "    with open(filepath_report, 'w') as file:\n",
    "        file.write(classifier_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    folder_path = os.path.join(\"../../../../cds-vis-data/Tobacco3482/\") # change to in for the final script\n",
    "    \n",
    "    list_of_images, list_of_labels = load_images(folder_path)\n",
    "    \n",
    "    preprocessed_images = preprocess_images(list_of_images)\n",
    "    #preprocessed_images = [(preprocess_images(image), label) for image, label in preprocessed_images]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
